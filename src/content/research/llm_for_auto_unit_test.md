---
title: "LLM改进自动化单元测试生成-综述总结"
date: "2025-12-14"
description: "GPT5.2总结"
tags: ["LLM", "单元测试", "自动化测试"]
---

大型语言模型（LLM）在代码生成领域的兴起为自动化单元测试生成带来了新的契机。这五篇论文共同关注**利用LLM改进单元测试生成**，探讨LLM方法与传统技术（如基于搜索的软件测试SBST、符号执行）的效果差异，以及如何弥补LLM生成测试时的不足。总体而言，这些研究强调：

- **研究背景**：手工编写高质量单元测试代价高昂，而传统自动化测试生成工具（如搜索式测试、符号执行等）虽可提供较高代码覆盖率，却往往生成难以阅读、缺乏语义意义的测试用例[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=program units. Manually writing high,empirical study to evaluate ChatGPT's)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=program units (e,ChatGPT is in unit test)。近年LLM在代码生成上表现出色，能生成更类似人类编写的有意义测试代码[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=coverage%2C they exhibit low readability,investigate the quality of its)。因此学界开始尝试LLM生成单元测试，但**其有效性和局限性尚不明确**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,still suffer from correctness issues)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Recent work has shown the,is in unit test generation)。主要挑战包括：LLM上下文窗口限制导致无法处理大型类/方法全部信息、可能产生不正确或不可执行的代码（编译错误、断言失败等）[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=responses resulted in a correct,symbol” during compilation and “AssertionFailedError”)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=quantitative analysis and a user,tests could be further improved)；以及LLM生成的测试覆盖不足某些复杂逻辑。
- **研究方向与方法类型**：这五篇工作涵盖*对比评估*、*LLM增强框架*和*混合策略*等多种方法：
  - Abdullin等（2025）的研究对比了**三种不同流派**：传统SBST（EvoSuite）、符号执行（Kex）和LLM（TestSpark）在单元测试生成中的效果[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=techniques such as search,based approaches)。
  - Chen等（2024）和 Wang等（2024）则提出了**改进LLM测试生成性能的框架**（ChatUniTest 和 HITS），通过新机制（上下文聚焦、方法切片等）提高LLM生成测试的覆盖率和正确性。
  - Lemieux等（2023）的CODAMOSA采用**混合策略**，将LLM与SBST结合，利用LLM帮助搜索算法突破覆盖瓶颈[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=help SBST escape its coverage,operations and fitness function to)。
  - Yuan等（2024）对ChatGPT生成单元测试进行了**深入评估**，并提出ChatTester框架用LLM自身迭代改进测试质量[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Inspired by our findings above%2C,assertions than the default ChatGPT)。
- **共同目标**：提升自动化测试生成的**覆盖率**（覆盖更多代码行和分支）与**可执行性**（生成正确、可编译运行的测试），并让生成的测试更贴近人类风格、易读易维护[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=coverage%2C they exhibit low readability,investigate the quality of its)。各工作皆尝试解决LLM生成测试存在的不同痛点，如：覆盖盲区、上下文限制、生成错误修复、复杂代码处理等。

接下来，我们将逐篇介绍每篇论文的内容、方法和贡献，并在最后进行横向对比分析不同方法的差异。

# 论文逐篇总结 ✏️

## 1. **“Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation” (Abdullin et al., 2025)**

**背景与动机**：该研究关注**LLM vs. 传统方法**在单元测试生成中的效果差异。此前一些工作已探索用ChatGPT 3.5生成测试并与EvoSuite比较，但存在评价不全面的问题，如可能的数据泄漏和缺少符号执行对比[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=We identify three critical limitations,partially tackle)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Limitation 2%3A No comparison with,did not consider these techniques)。为此，作者识别了三大限制：(**1**)评估数据集常包含LLM训练过的项目（如Defects4J），可能出现数据泄漏；(**2**)以往研究仅对比了LLM与SBST，忽略了符号执行方法；(**3**)LLM的**非确定性**缺乏充分统计分析（以往只跑单次，没有多次运行做统计检验）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=We identify three critical limitations,partially tackle)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Limitation 3%3A Lack of statistical,30%2C 47] statistically)。这些问题可能导致对LLM真实能力认识偏差。因此，作者决定在更严谨条件下**公平对比**三种方法。

**方法与框架**：论文设计了一套**自动化测试生成评估流程**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=)。选取三个代表性工具：**EvoSuite**（搜索式测试，SBST）、**Kex**（符号执行）以及**TestSpark**（LLM方法，JetBrains开源的IntelliJ插件[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=TestSpark ,improve the initial LLM response)）。为避免数据泄漏，评估使用**GitBug Java数据集**（收集自2024年最新修复的缺陷项目）作为测试基准[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=techniques such as search,based)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=We identify three critical limitations,partially tackle)。TestSpark调用多种LLM模型生成测试，包括ChatGPT-4、开源Llama等，以比较不同LLM性能[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=In this paper%2C we conduct,a dataset of recent Java)。每种工具对每个类运行**10次**（LLM多次对话会话）以平抑随机性，并采用统计方法比较结果[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=performance between alternative LLMs when,used in prior SBST studies)。评价指标涵盖**执行度量**（如编译成功率、代码覆盖率、变异测试分数、发现缺陷能力）以及**被测代码特征**（类复杂度、依赖等）对性能的影响[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Correlation between coverage metrics and,and Java Docs%2C and SLOC)。

**实验设置与结果**：在GitBug数据集上比较结果显示：**SBST和符号执行在覆盖率上领先**——EvoSuite平均行覆盖率约46.4%、Kex约38.6%，而TestSpark-ChatGPT4仅38.1%[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=match at L692 In terms,However%2C we)。特别是**分支覆盖**上EvoSuite更胜一筹（分支覆盖34.9%，比LLM高约10个百分点）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=match at L692 In terms,However%2C we)。但有意思的是，**LLM生成的测试在变异得分（Mutation Score）上远高于传统方法**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)。也就是说，尽管LLM测试覆盖较少代码，但它们更善于“杀死”变异体，体现了对代码语义更深入的理解[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)。在**缺陷检测**能力方面，LLM生成测试表现不佳，EvoSuite和Kex生成的测试捕获更多已知缺陷[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)。**编译成功率**上，Kex最高（接近99.99%几乎所有生成测试都可编译运行），EvoSuite和LLM略低，LLM尤其因语法/依赖错误时有编译失败案例[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=These graphs give us multiple,out of all three)。**特征分析**发现：所有工具的性能都受被测类复杂度和内部依赖影响，类越复杂覆盖率越难提升。其中LLM方法对**被测类规模**尤为敏感，类很大时覆盖率下降显著[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=suggesting that LLMs provide a,sensitive to the CUT size)。

**贡献与创新**：本研究是首个同时比较SBST、符号执行和LLM测试生成的综合研究，**填补了多方法对比的空白**。主要贡献包括：开放了可扩展的测试生成评估管线工具[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=)；基于覆盖率、变异分数、编译率等**多维指标**全面分析三类技术优劣[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=An extensive analysis of three,mutation score of generated tests)；以及将代码复杂度等特征与各方法效果关联的协方差分析，揭示了不同技术在类复杂场景下的优势和脆弱点[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Correlation between coverage metrics and,and Java Docs%2C and SLOC)。例如，他们指出LLM测试对大规模类“力不从心”，提示未来可结合传统技术弥补此劣势[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=w,sensitive to the CUT size)。

**不足与改进**：结果显示当前**LLM方法仍无法取代传统工具**：覆盖率方面明显落后，且缺陷检测不如进化或符号方法。这表明LLM生成测试虽然语义相关性强，但**缺乏探索广度**（未覆盖足够路径）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)。作者建议未来**融合多种技术**（“不同AI部族”联合）或改进LLM提示，以兼顾广度和深度[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=This paper is organized as,while Section VIII concludes the paper)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=II)。另外，LLM在大类上的失效和对上下文长度敏感是一大局限，需要研究**如何扩充LLM上下文窗口或拆分任务**来提升其针对大型类的覆盖效果。总的来说，本研究为后续工作指明了方向：可以考虑**混合策略**（详如CODAMOSA等）或者改进LLM框架（如ChatUniTest、HITS等）来取长补短。

## 2. **“ChatUniTest: A Framework for LLM-Based Test Generation” (Y. Chen et al., 2024)**

**研究背景与动机**：ChatUniTest致力于**提高LLM自动生成单元测试的实用性**。先前已有AthenaTest、A3Test等深度学习模型用于测试生成，但它们**缺乏与模型的交互**，无法在生成错误时进行修复[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=based tool%2C pretrains a BART,potential to transform the situation)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=results in terms of correctness,learning model%2C leading to most)。例如，AthenaTest将测试生成视为代码翻译任务，直接从方法生成测试，**仅约20%\**生成结果是正确可用的[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=AthenaTest[5 ] generates human,both models fail to correct)。A3Test引入了一些改进（如断言知识、命名一致性检查）提升了正确率和方法覆盖，但仍无法在生成后\**纠正简单错误**[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=unit tests by fine,in the generated unit tests)。另一方面，传统工具如EvoSuite虽覆盖率高，但生成的测试**可读性差**、断言有限，不像人写的测试[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=developed,its dependencies within the pre)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=tool and deep learning unit,upon AthenaTest by incorporating assertion)。**ChatGPT**的出现带来了希望：它可以在自然语言对话中，根据上下文要求生成解释良好、结构清晰的测试。然而，初步尝试发现ChatGPT存在两个关键局限：[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=Motivation,Suggesting an adaptive)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=limitation,effectiveness in generating unit tests)(1) **上下文长度限制**：如GPT-3.5只有4096 tokens上下文，导致对于大型类或依赖多的模块，无法输入全部相关信息，ChatGPT可能遗漏关键信息，**生成不完整测试**[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=responses resulted in a correct,symbol” during compilation and “AssertionFailedError”)。(2) **无验证反馈**：ChatGPT不会自行编译运行它生成的代码，因而生成的测试常有错误（如引用未定义变量、断言不正确等）却**无机制自我修复**[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=limitation,effectiveness in generating unit tests)。作者动机是**针对这两点改进**，发挥ChatGPT生成“类人”测试代码的优势，同时克服上下文和正确性瓶颈。

**核心方法（框架设计）**：ChatUniTest提出了一个**“生成-验证-修复 (Generation-Validation-Repair)”**的流程[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatGPT,Repair to address challenging errors)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=defined maximum prompt token limit,Repair to address challenging errors)。具体包括：

- **自适应焦点上下文机制（Adaptive Focal Context）**：将“被测单元”定义为**单个方法**。在生成测试前，首先**解析项目代码**，收集待测方法的关键信息，如所在类的声明、相关依赖、方法实现、调用关系等[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=the project%2C extracting essential information%2C,and A3Test in focal method)。然后算法根据预定义的最大token长度生成**焦点上下文**：优先放入必要的信息（目标方法代码、依赖方法签名等），剔除无关内容，使上下文既**尽量完整**又**不超长度**[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatUniTest introduces an Adaptive Focal,avoiding truncated responses from ChatGPT)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=the project%2C extracting essential information%2C,and A3Test in focal method)。这样ChatGPT能够获取生成测试所需的关键细节，又避免提示超长被截断[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatUniTest introduces an Adaptive Focal,avoiding truncated responses from ChatGPT)。
- **生成 (Generation)**：将上述焦点上下文嵌入prompt模版，提示ChatGPT生成针对该方法的JUnit测试代码[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=on unit test generation has,based language models)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=treating unit test generation as,in the generated unit tests)。ChatGPT返回后，从回复中提取**原始测试代码**。
- **验证 (Validation)**：对生成的测试进行**语法解析、编译**，并执行看是否通过[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=defined maximum prompt token limit,Repair to address challenging errors)。这里ChatUniTest集成了编译器和JUnit执行器，自动捕获错误类型：如编译期的`符号未找到`错误，或运行期的`断言失败`等[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=limitation,effectiveness in generating unit tests)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=limitation,Suggesting that a validation)。
- **修复 (Repair)**：如果测试未通过验证，则进入双阶段修复：
  1. **规则驱动修复**：应用预定义规则自动修改常见简单错误，如**添加缺失的import**、修正简单语法错误等[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=defined maximum prompt token limit,Repair to address challenging errors)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)。这一步处理显式易改的问题。
  2. **ChatGPT交互修复**：对于复杂错误（规则无法修复），构造新prompt提供**错误信息+当前错误的测试代码+上下文**，请求ChatGPT给出修正后的测试[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=component includes a parser%2C compiler%2C,If a test remains)。ChatGPT基于错误提示通常能修复约一半的棘手错误[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatUniTest can effectively repairs syntactic,based repair component)。这一过程可多轮循环：修复后重新验证，若还有新错误则再次交由ChatGPT，直到成功或达到设定轮数[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=addresses simple errors such as,If a test remains)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=constructs a prompt with an,it is marked as deprecated)。若多轮仍未修复，则标记该测试生成失败，放弃此单元。

此外，ChatUniTest还开发了**Core核心库**实现上述流程，以及**Toolchain工具链**（如IntelliJ IDEA插件），方便集成到IDE中实际使用[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=validation,https URL%2C and the demo)。

**实验设置**：作者在若干Java开源项目上评估ChatUniTest。根据论文描述，他们选取了4个不同行业领域的Java项目进行比较（包括部分含复杂依赖的项目）[researchgate.net](https://www.researchgate.net/publication/382170482_ChatUniTest_A_Framework_for_LLM-Based_Test_Generation#:~:text=Generation www,from different domains)。基线对比工具有**TestSpark**（另一LLM测试工具）和**EvoSuite**。另外还与早期学习式工具AthenaTest、A3Test进行了对比分析。评价指标主要是**行覆盖率**、**分支覆盖率**，以及**生成测试的正确率**等。同时进行了**用户研究**，请开发者主观评价生成测试的有用性。

**主要结果**：ChatUniTest取得了**业界领先**的覆盖率和可用性表现：

- 在项目级比较中，ChatUniTest在**一半的测试项目上覆盖率优于EvoSuite和TestSpark**，并且总体平均**行覆盖率最高**[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=implements core workflow%2C complemented by,available at this https URL)。据报告，其平均行覆盖达89.36%，分支覆盖90.6%，相比EvoSuite的80.0%和86.6%有明显提升[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=2011 ,on their datasets)。尤其在一些有复杂逻辑的方法上，ChatUniTest显著胜出。
- 与深度学习基线相比，ChatUniTest**有效生成了更多正确测试**。一次生成尝试中约有30%的测试能通过全部验证成为“正确测试”[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=unfixed after several validation and,it is marked as deprecated)（相比ChatGPT原生不到20%的成功率有提高[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=Motivation,Suggesting an adaptive)）。其生成的**断言丰富**、还能合理使用**模拟对象（mock）和反射**调用以覆盖目标行为[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=compile errors%2C followed by ChatGPT,reflection to achieve test objectives)。
- **覆盖率细节**：对10个测试项目，ChatUniTest在7个项目上分支覆盖率超过EvoSuite，在8个项目上行覆盖率更高[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=Evaluation Results,of 18 methods in the)。相较最新LLM工具A3Test，ChatUniTest生成的**焦点方法覆盖率**（即被测方法本身的代码覆盖）达到79.7%，远超AthenaTest的43.8%和A3Test的46.8%[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=compared to AthenaTest%2C achieving a,method line coverage as well)。例如，在Defects4J基准Lang-1项目的`NumberUtils`类18个方法中，ChatUniTest在16个方法上取得了更高的覆盖率[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=positive outcome in terms of,method line coverage as well)。
- **正确性和修复**：ChatUniTest通过**Repair机制**修复了大约**50%\**的复杂错误[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)。对生成的错误测试，大部分简单问题（如引入缺失）都能规则修复，其余通过ChatGPT多轮修复解决了一半左右\**棘手错误**[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)。最终显著提高了生成测试的**编译通过率**和**可运行性**。
- **用户研究反馈**：开发者普遍认为ChatUniTest生成的测试**具有实用价值**[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=effectiveness evaluation reveals that ChatUniTest,https URL%2C and the demo)。测试可读性好、结构清晰，有助于理解代码意图；一些参与者表示愿意将该工具用于日常测试开发辅助，可见其在工业中的潜力。

**论文贡献与创新点**：

- **第一个基于ChatGPT的自动单元测试生成工具**：ChatUniTest率先将对话LLM引入测试生成场景，构建了完整工具链并**开源**供社区使用[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=effectiveness evaluation reveals that ChatUniTest,available at this https URL)。
- **自适应焦点上下文算法**：根据被测方法动态组装最相关的代码片段作为提示，**避免上下文截断**问题[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatUniTest introduces an Adaptive Focal,avoiding truncated responses from ChatGPT)。这是解决LLM上下文限制的创新尝试，使ChatGPT聚焦于当前方法的必要信息，提高了有效生成的概率。
- **两阶段修复机制**：融合**规则**和**LLM对话**修复错误，大幅提升生成测试的正确率[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)。尤其是利用ChatGPT自身来修复ChatGPT代码，在当时是独特的设计，验证了LLM可以迭代自我改进。
- **更高的覆盖和可读性**：实验证明ChatUniTest生成的测试不但覆盖率超过传统工具，在**测试语义合理性**上也更接近人编写的测试[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=compile errors%2C followed by ChatGPT,reflection to achieve test objectives)。例如会生成恰当的断言和变量名，弥补了EvoSuite这类工具测试可读性差的不足[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=tool and deep learning unit,upon AthenaTest by incorporating assertion)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=test cases based on their,with poor understandability and maintainability)。

**不足或待改进**：尽管ChatUniTest整体表现优异，仍存在一些局限：

- **生成失败率**：约70%的生成尝试需经历修复甚至最终失败，这意味着完全自动化程度还有待提高[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=unfixed after several validation and,it is marked as deprecated)。特别是对于**非常复杂或大型的类**，哪怕有焦点上下文，ChatGPT仍可能无法覆盖所有分支条件，导致覆盖率骤降（作者发现对于圈复杂度高的复杂方法，ChatUniTest的覆盖率相较简单方法下降了40-60个百分点[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Table 1,61.11)）。
- **多轮交互成本**：修复机制依赖多轮ChatGPT调用，会增加时间和计算成本。在资源受限或需要大规模测试时，此方法的效率和费用需考虑优化（例如减少不必要轮次、并行生成等）。
- **局限于Java**：目前框架实现在Java项目，使用了Java解析和JUnit等，如何推广到其他语言生态（如Python、C#）需要额外工作。
- **对LLM依赖**：ChatUniTest性能高度依赖于ChatGPT的能力，若LLM的知识或推理不足，框架效果也受限。未来可考虑结合LLM与传统分析进一步提升稳健性。

总体而言，ChatUniTest为LLM测试生成树立了一个新范式，通过上下文定制和反馈循环显著提升了生成质量，适合初学者了解如何**工程化地应用LLM**来辅助软件测试。

## 3. **“CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models” (Lemieux et al., 2023)**

**研究背景与动机**：CodaMOSA关注的是**融合LLM与搜索算法**来提高自动测试覆盖率的问题。背景是传统的**搜索式测试生成（SBST）\**虽强大，但经常遭遇\**覆盖停滞**（coverage plateau）[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=tool can generate test cases,7)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=plateaus like these—where mutated test,program might expect a particular)：算法在随机或进化生成测试过程中，可能卡在局部最优，无法产生引入新行为的输入。例如作者举了个Python函数`bump_version`：SBST随机生成的字符串几乎不可能碰巧满足版本格式要求来走进更深逻辑，导致后续变异也难逃出这一盲区[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=fitness relates to higher coverage,because the probability of mutating)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=tool can generate test cases,7)。研究显示这种适应值停滞现象在测试生成中**相当常见**[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=necessary to exercise any more,7)。**核心问题**在于：SBST**缺乏对程序预期行为的知识**，在需要特定格式输入或调用特定序列时束手无策[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=One core problem leading to,produce these expected test cases)。作者假设：如果能适时提供一些“合理的”测试用例，帮助SBST跳出停滞，覆盖率就能继续提高[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We hypothesize that if we,SBST escape its coverage stalls)。LLM（如OpenAI Codex）具备根据大量语料**生成看似合理代码片段**的能力，或许可以用来**辅助SBST突破瓶颈**[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=Recently%2C large language models ,Codex to generate tests for)。因此，研究动机是结合LLM的智慧与SBST的系统探索，取长补短提升覆盖。

**核心方法**：提出**CodaMOSA**（名称来自“Code+MOSA”，MOSA是一种多目标搜索算法）。它的流程是：**在SBST过程中监控覆盖进展**，一旦检测到覆盖率长时间未提高（出现plateau），就**暂缓SBST的变异搜索**，转而调用LLM生成新测试[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=help SBST escape its coverage,operations and fitness function to)。

具体实现：

- 基于现有SBST框架（作者使用Python的Pynguin工具）启动常规的遗传算法测试生成[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=code,character sequences generated by Codex)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=are integrated into test cases,CODAMOSA on 486 Python modules)。同时持续追踪每轮的覆盖率提升。
- 当发现若干代迭代后覆盖率不再增长（停滞），CodaMOSA分析**当前被测程序中覆盖率低的函数或代码片段**[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=monitors its coverage progress,operations and fitness function to)。这些往往是SBST难以触及的部分（例如需要特殊输入）。
- **调用LLM（OpenAI Codex）**：针对选出的低覆盖目标，向Codex发送请求，**让它生成额外的测试用例**[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=monitors its coverage progress,operations and fitness function to)。提示包含函数签名、实现和文档等上下文，让Codex产出能调用该函数的测试代码[dl.acm.org](https://dl.acm.org/doi/10.1109/icse48619.2023.00085#:~:text=,covered functions)。
- **解析与整合**：Codex返回的是完整Python代码字符串。CodaMOSA设计了**反序列化模块**，将这段字符串解析为Pynguin内部的测试表示[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=low coverage,challenge%3A to make the search)。这是一个难点，因为LLM生成代码可能含有SBST框架不允许的新语法或API调用。CodaMOSA巧妙地**筛选融合**：如果LLM提供的新函数调用或语法有助于覆盖且框架能执行，就把它整合进测试集合，使搜索空间扩展[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=deserializes the raw character sequences,on the other hand%2C generates)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=tractable expansion of SBST’s search,on compared to our)；否则可以舍弃不合理部分，以保持搜索可控。
- 将LLM生成的测试用例**并入SBST的种群**，作为新的个体参与后续遗传变异和评估[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=low coverage,challenge%3A to make the search)。SBST随后继续运行，从这些人为“注入”的高质量种子出发，可能探索到更多路径。
- 如此交替进行：SBST → 停滞 → LLM生成 → SBST 继续，直到达到时间或资源上限。

**实验设置**：CodaMOSA针对**Python语言**实现，并在486个Python模块上进行了大规模测试（这些模块来自不同开源项目，提供广泛代表性）[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=are integrated into test cases,on compared to our)。将CodaMOSA与两类基线对比：纯SBST（即Pynguin单独运行）和纯LLM（只用Codex生成测试，不做搜索）。比较指标主要是**代码覆盖率**（行覆盖）。此外对LLM生成的典型案例做了定性分析。

**主要结果**：CodaMOSA在覆盖率上展示出**显著优势**：

- **覆盖率提升**：与SBST基线比，CodaMOSA在486个基准中有173个显著提高了覆盖率，而只有10个略有下降[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,The average magnitude of)。与纯LLM比，279个项目覆盖率提升，仅4个下降[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,The average magnitude of)。**提高的案例远多于降低的**，说明融合策略总体有效。
- **增益幅度**：覆盖率提升的平均值约**+10%**（对比SBST）和**+9%**（对比LLM），而覆盖率下降的平均幅度仅-3%～-4%[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=than it reduces coverage ,In)。这表明即便CodaMOSA未必在每个项目都超越，但其改进通常远大于偶尔的退步[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)。整体看来，它在**绝大多数项目上不低于单用SBST/LLM**，并在大量项目上带来可观增益[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)。
- **典型案例分析**：CodaMOSA成功场景中，LLM常能生成SBST难以碰到的特定测试。例如Codex能**产出特定格式字符串、调用隐藏的库函数**，或引入新的构造（如对象初始化序列）来触发深层次行为[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=the coverage increase is also,functions%2C and introduce syntactical 1)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=Recently%2C large language models ,Codex to generate tests for)。这些**特别输入**帮助SBST跳出了原来的停滞区域。例如`bump_version`场景下，LLM生成了合法版本字符串，从而让后续遗传算法可以围绕这个新起点进一步覆盖其它分支[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=test%2C bump version%2C increments program,test cases are unlikely to)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We hypothesize that if we,SBST escape its coverage stalls)。
- 反之，在少数覆盖下降的情况，分析发现可能是**LLM生成了干扰项**或无效测试，浪费了一些搜索资源。不过这种情况很少见且影响不大。

**贡献与创新点**：

- **提出首个LLM与SBST结合的测试生成方法**：CodaMOSA证明了LLM和传统搜索并非对立，而是可以合作。通过**监测停滞点**智能调用LLM，这是以前SBST框架所不具备的机制，让测试生成**更具适应性**。
- **技术难点突破**：设计了LLM生成代码到SBST内部表示的转换方案，解决了LLM自由生成和SBST严格格式要求之间的矛盾[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=low coverage,challenge%3A to make the search)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=present in a test case,that CODAMOSA achieves statistically significantly)。这一**反序列化整合算法**可视为在约束搜索空间与LLM丰富输出之间寻找平衡的有益探索。
- **大规模实证**：在近500个实际模块上验证方法，有统计显著的改进效果[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)。如此大规模评估凸显了该方法的**通用有效性**，增强了结论的说服力。
- **混合新范式**：CodaMOSA开创了一种混合范式，为后续研究指明方向——将**LLM作为工具调用**而非完全替代，以在人机知识之间取得最佳组合。作者也讨论了如何利用覆盖信息指导LLM（后继有工作CoverUp改进了这一点[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=is to fall back to,design new strategies to combine)），体现了**反馈驱动LLM**的新思想。

**不足与局限**：

- **多模型交互成本**：CodaMOSA在停滞时调用Codex API，多次交互可能带来时间和计算成本（以及实际使用中的金钱成本）。在离线环境或成本敏感场景，需要衡量这一开销。不过作者的实验显示，只有在必要时才调用LLM，相对收益较高。
- **生成代码质量控制**：虽然有反序列化过滤，但LLM可能生成无用甚至有害的测试用例。极少数项目覆盖率下降表明**引入的测试并不总是有益**。未来可考虑更精细的判断何时/何处调用LLM、如何识别并丢弃无效生成，以**减少干扰**。
- **适用范围**：当前实现针对Python，而且高度依赖Pynguin框架。不同语言或不同SBST实现如何结合LLM需要验证。比如Java的EvoSuite是否也能类似结合，需要应对Java语法和工具链的特殊性。
- **未关注测试语义**：CodaMOSA主要以覆盖率为目标，对生成测试的**断言正确性**、**可读性**未深入讨论。LLM生成的测试虽形式合理，但其断言可能不严谨（Codex未必理解函数语义，只是合成调用）。因此在**缺陷检测能力**上是否提升，这超出了本文范围但值得后续评估。

综上，CodaMOSA作为**混合智能**的代表，有效缓解了SBST的覆盖盲区问题，对复杂输入需求的代码取得更高覆盖率[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)。它表明将LLM融入现有测试生成流程是切实可行且收益可观的，为提高自动测试生成的覆盖率提供了一条新思路。

## 4. **“HITS: High-coverage LLM-based Unit Test Generation via Method Slicing” (Z. Wang et al., 2024)**

**研究背景与动机**：HITS关注**LLM在处理复杂方法时的覆盖率不足**问题。背景是虽然LLM（如ChatGPT等）已显示出较强代码理解和生成能力，在许多Java项目上能自动生成不错的测试[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=,generate test cases slice by)。但在**复杂方法**（通常指**圈复杂度>10**、包含多层条件和循环的函数）上，现有LLM测试生成效果很差[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=tests for Java projects,easier for the LLM to)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=challenging for existing automatic unit,perform poorly on complex methods)。复杂方法要求测试用例**多样化**才能覆盖其众多分支和行。然而目前LLM测试工具通常是**把整个方法直接提供给模型**，缺乏对输入空间的引导分析，LLM**难以推理出穷尽所有条件的测试输入**[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=and loops%2C requiring the test,We build a dataset comprising)[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=method,LLMs and the typical SBST)。结果往往只覆盖了部分路径，大量代码行和分支未触及[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=and loops%2C requiring the test,We build a dataset comprising)。例如，ChatUniTest在作者评估中，对一般方法能达到80-90%覆盖，但针对复杂方法覆盖率骤降到仅30-50%[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Table 1,61.11)。这些**高复杂度方法**通常也是高风险代码（潜藏更多bug），因此提高其测试覆盖非常重要[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=challenging for existing automatic unit,perform poorly on complex methods)。鉴于此，作者提出运用**“分而治之”**思想：把复杂方法拆解成较小片段，分别生成测试来覆盖，每片相对简单，最后合并提升整体覆盖[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)。这一**方法切片（method slicing）**策略的直觉是：缩小LLM每次分析范围，让它逐片考虑不同部分逻辑，降低难度并逐步覆盖所有路径。

**核心方法**：HITS方法可概括为：**静态分析 + 切片分解 + 分片测试生成 + 结果合并**。步骤如下：

- **复杂方法识别与预处理**：首先通过**静态分析**解析目标方法的抽象语法树（AST）等，提取其**结构信息**和**依赖**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=slice of the focal method,to instruct the large language)。根据代码逻辑，将方法按照**问题-解决步骤**划分为多个**代码片段（slice）**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=To address the challenge%2C we,execution paths in the previous)。作者将每个slice定义为一组连续的语句，代表该方法实现中的一个阶段性任务[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=decomposed from the original method,cover%2C only one feasible execution)。这个划分可由LLM（ChatGPT）初步给出，再由经验程序员审校，以确保合理[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Figure 1 illustrates decomposing a,When focusing)。例如在Commons-CLI库的`HelpFormatter.renderOptions()`方法中，ChatGPT将其分为三段，每段处理选项列表渲染的不同步骤[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Figure 1 illustrates decomposing a,When focusing)。图示中不同背景色标注出三个slice，分别对应处理选项列表是否为空、遍历选项长度描述等子任务[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Figure 1 illustrates decomposing a,When focusing)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=and returns the rendered content,coverage)。
- **逐片生成测试**：对于每个slice，采用Chain-of-Thought（CoT，链式思维）提示模式引导LLM**逐步规划并生成**测试用例[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=slice of the focal method,to instruct the large language)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=language model to understand the,to instruct the large language)。具体而言，HITS会向LLM提供该slice的代码以及从静态分析得来的上下文（如这个slice涉及的变量状态、前置条件等），然后通过提示让LLM思考“要覆盖这段代码需要怎样的输入和调用”[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=slice of the focal method,to instruct the large language)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=decomposed from the original method,cover%2C only one feasible execution)。LLM据此生成针对该slice的一个或多个JUnit测试。由于slice较小，LLM**只需考虑局部分支**：例如slice 2涉及检查列表是否为空，那么LLM只需生成一个有空列表、一个非空列表的测试就能覆盖相关分支[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=and returns the rendered content,coverage)。相比考虑整个方法，难度大大降低[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=To address the challenge%2C we,execution paths in the previous)。
- **测试合并**：将各slice生成的测试用例合并起来，形成完整的测试套件，力求**覆盖原方法所有行和分支**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=To address the challenge%2C we,execution paths in the previous)。不同slice的测试可能有重复或冲突，但一般因针对不同逻辑部分而互补。
- **自动修复与验证**：HITS也包含一个**修复（fixer）组件**。对LLM生成的每个测试执行编译和运行验证，自动修正一些错误，确保最终测试套件可成功运行[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L234 includes a,our method in Figure 2)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L234 includes a,our method in Figure 2)。修复机制类似ChatUniTest，包含规则修复和LLM交互修复，不过论文未详述细节，重点在切片策略上。

**实验设置**：作者构建了一个**数据集**，收集了现有研究（如ChatUniTest、ChatTester等）使用过的项目中**复杂方法**作为评估对象[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。在这些复杂方法上比较HITS与多种基线，包括：几种LLM测试工具（ChatUniTest、ChatGPT直接生成等）和传统SBST工具EvoSuite。[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L530 According to,results on the remaining 3)提到一共评估了10个项目，其中HITS与其它方法在每个项目的平均覆盖率作比较。

**主要结果**：

- **覆盖率显著提升**：HITS在**行覆盖率和分支覆盖率**上**全面超越**其它LLM方案以及EvoSuite[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。平均而言，HITS在测试项目中取得了最高的覆盖分数：在10个项目里，有7个项目HITS的覆盖率排名第1，另外3个项目排名第2，仅次于极个别情况[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=According to Table 4 and,results on the remaining 3)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=dive deep into the results%2C,results on the remaining 3)。综合所有项目，HITS的平均行/分支覆盖率高于次佳方法一个明显幅度[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L530 According to,results on the remaining 3)。例如，对Commons-collections项目等大规模库，HITS略逊于LLM基线（可能因这些库LLM训练中见过，LLM可以直接“背答案”[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L535 Compared with,Thus%2C ‘recalling’ various test cases)），但在大多数项目HITS领先。
- **对比传统工具**：在复杂方法集上，HITS的覆盖率**也超过EvoSuite**（SBST代表）[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。尽管EvoSuite在一般情况下覆盖率高，但对这些高复杂度方法也力不从心，HITS凭借LLM智能和切片策略取得更好的结果。作者指出，个别项目如Datafaker中EvoSuite仍表现不俗，暗示传统方法在某些场景下有优势，但总体HITS在复杂方法上更胜一筹[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L542 We have,the second best method is)。
- **测试可执行性**：HITS生成的测试**通过率较高**。他们统计了各LLM方法生成的测试数量与成功运行比例，发现HITS的平均**编译/运行通过率**领先于其它LLM基线[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Additionally%2C we report the number,based)。值得注意的是，他们发现“通过率低会导致覆盖率低，但通过率高未必保证高覆盖”[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=match at L581 shows that,Designing)——HITS兼顾了两者，通过**切片策略**提高了覆盖率，同时通过修复机制保证了生成测试大多可运行。这相当于既解决“测不到”又解决“测不对”的问题。
- **举例分析**：论文通过前述HelpFormatter的例子展示了切片的效果：针对slice的测试只需考虑局部情形，例如slice2重点考虑optList为空与否、slice3考虑optList的单个元素属性等[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=and returns the rendered content,coverage)。LLM不需顾及所有前序路径，只要确保前面slice已提供达到当前slice的前提即可[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=by simplifying the large language,the desired test inputs easier)。这种**聚焦**令LLM更容易产出满足分支需求的输入。例如某slice要求一个对象的特定状态，LLM就生成恰好营造该状态的代码，而不必处理方法其他部分的干扰。

**论文贡献与创新点**：

- **方法级切片测试生成**：首次提出将**程序 slicing 思想**用于LLM测试生成，将复杂方法按逻辑步骤分解，使LLM聚焦于子问题[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)。这一创新大幅提升LLM生成高覆盖测试的能力，被证明比“一次性生成整方法测试”有效得多[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。
- **Chain-of-Thought 提示**：将CoT用于单元测试场景，引导LLM逐步思考如何测试每个slice[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=slice of the focal method,to instruct the large language)。CoT通常用于问答推理任务，此处巧妙应用在代码测试上，帮助LLM“分步”解决测试生成问题，体现了提示技巧的创造性。
- **复杂方法测试数据集**：构建了专门的复杂方法基准并验证各种方法表现，**填补了以往平均指标掩盖下复杂场景细节的空白**。结果强调了现有LLM方法在复杂代码上的弱点，并提供了HITS作为有效解决方案。
- **高覆盖率实证**：HITS在多项目上超过90%的覆盖率，刷新了LLM测试生成的纪录[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=2011 ,on their datasets)。证明只要策略得当，LLM完全有潜力达到甚至超越传统方法的覆盖水平。

**不足与局限**：

- **适用范围局限**：HITS的优势主要体现在**高圈复杂度方法**。对普通简单方法，直接用LLM生成已能较好覆盖，切片反而增加流程复杂度。因此HITS适合在检测到某方法复杂且覆盖率低时启用，如何**自动判定应用条件**是工程上需考虑的。
- **方法切片自动化**：目前切片依赖ChatGPT初步给出再人工调整[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Figure 1 illustrates decomposing a,When focusing)。完全自动根据代码结构划分slice可能存在挑战，尤其对更复杂逻辑或无明显阶段的函数。未来或可借助程序分析技术改进自动切片算法。
- **额外开销**：对每个slice分别生成测试，可能导致**多倍的LLM调用**。在方法包含很多slice的极端情况下，效率和成本值得注意。但相比提高的覆盖率和质量，这可能是可以接受的权衡，并可通过并行等优化减缓。
- **LLM质量依赖**：HITS仍然依赖LLM本身生成有意义的测试。如果LLM对某slice逻辑误解，可能给出错误测试。不过由于slice简单，此风险较小。作者也引入了结果修复，但未来更强的LLM会进一步提高效果。

总的来说，HITS针对LLM测试生成的**覆盖率瓶颈**提供了一个优雅的解决方案。在复杂代码测试自动化方面迈出重要一步，也启发后续研究考虑更多**基于程序结构的提示策略**，提高LLM在软件工程具体任务上的表现[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)。

## 5. **“No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation” (Z. Yuan et al., 2024)**

**研究背景与动机**：这项研究首先系统评估了**ChatGPT**直接用于单元测试生成的能力，并进一步提出改进策略。动机源于观察：传统自动生成方法虽能给出一定覆盖率的测试，但因可读性差、语义单薄，开发者往往不愿采用[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=program units (e,ChatGPT is in unit test)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=In this work%2C we perform,Our findings indicate)。而LLM尤其ChatGPT擅长生成**更接近人编写**的代码[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=coverage%2C they exhibit low readability,investigate the quality of its)。ChatGPT结合了强化学习人类反馈（RLHF）等技术，模型对人意图的对齐更好，按理在生成易读、规范的测试方面**很有潜力**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,still suffer from correctness issues)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Recent work has shown the,is in unit test generation)。但问题在于，当时尚不清楚ChatGPT实际生成测试的质量如何：能达到多高覆盖？有多少错误？开发者是否喜欢这些测试？因此作者进行**首个全面实证**来回答这些问题[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=and reinforcement learning%2C has performed,written tests by achieving)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=In this work%2C we perform,be very promising if the)。在发现ChatGPT存在不足后，他们探讨能否**利用ChatGPT本身来改善它输出的测试**，提出ChatTester框架。

**评估方法**：研究收集了**1000个Java方法**及其完整项目环境作为评估数据集[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Study,quality of the returned tests)。采用一个**基础提示**方案与ChatGPT API交互，每次提供：测试任务说明 + 待测方法代码 + 相关上下文，让ChatGPT生成对应的JUnit测试[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Study,quality of the returned tests)。评价涵盖：

- **正确性 (Correctness)**：统计生成测试的语法正确率、编译通过率和执行通过率（即是否所有断言通过）[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=quantitative analysis and a user,tests could be further improved)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)。并对错误类型分类（例如未编译因变量未定义、断言失败等）。
- **充分性 (Sufficiency)**：分析通过的测试在覆盖率和断言方面的充分性，即相比开发者编写的是否达到类似覆盖，断言是否合理[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=quantitative analysis and a user,tests could be further improved)。
- **可读性 (Readability)**：进行开发者问卷，比较ChatGPT测试与人写测试在可读性上的差异。
- **可用性 (Usability)**：让开发者评价是否愿意使用ChatGPT生成的测试，有何改进建议等。

**主要评估发现**：

- **正确性低下**：ChatGPT生成的测试**大部分存在问题**。具体数字是：仅约**24.5%\**的生成测试能够无错误地编译并通过执行，其余接近75%都失败[mingwei-liu.github.io](https://mingwei-liu.github.io/assets/pdf/arxiv2023-chattester.pdf#:~:text=terms of correctness%2C sufficiency%2C readability%2C,tests could be further improved)[mingwei-liu.github.io](https://mingwei-liu.github.io/assets/pdf/arxiv2023-chattester.pdf#:~:text=correctness issues%2C including diverse compilation,propose CHATTESTER%2C a novel ChatGPT)。错误中，以\**编译错误**居多（例如引用了不存在的方法/变量、类型不匹配等），占所有错误的约一半以上[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)。其次是**运行失败**（主要由于断言不正确导致AssertionError），约占错误的近1/5[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)。这些说明ChatGPT经常**幻觉出**不存在的接口或断言条件，与真实代码不符。
- **覆盖率和质量**：令人惊喜的是，那**四分之一通过的测试**质量相当高——它们的**覆盖率可与手写测试媲美**，平均覆盖了被测方法的大部分代码，与EvoSuite等生成的测试持平[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=usability,with ChatGPT could be very)。同时这些通过的测试**格式优雅、语义清晰**，在可读性上几乎和开发者编写的看不出区别[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=usability,with ChatGPT could be very)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=,tests could be further improved)。在一些案例中，开发者甚至更喜欢ChatGPT生成的测试，因为它写的断言和注释更详尽[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=usability,tests could be further improved)。这表明**一旦ChatGPT生成正确**，其结果是非常有价值且“类人”的。问题只是**正确率太低**了。
- **开发者反馈**：用户研究显示，开发者普遍认为ChatGPT测试**易读性好**，但对其**可靠性存疑**。手工审查ChatGPT错误测试发现，很多错误本可通过简单修改纠正，这启发作者考虑自动修复思路。

**ChatTester改进方法**：基于上述发现，作者提出**ChatTester框架**来**提高ChatGPT生成测试的正确率**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。ChatTester继续采用ChatGPT，但分两阶段：

1. **初始测试生成 (Initial Test Generator)**：不用一次性让ChatGPT直接给最终测试，而是**先引导ChatGPT理解被测代码意图**，再生成测试。具体做法是拆分prompt：首先一个“意图提示”（intention prompt），要求ChatGPT总结被测方法的功能/目的；接着一个“生成提示”，把方法代码和ChatGPT自己生成的意图说明一并提供，请其依此编写测试用例。这相当于让模型**先思考**再动手，减少凭空臆测。有点类似Chain-of-Thought的思路，把方法语义显式化，期望提高测试的相关性和正确性。
2. **迭代测试优化 (Iterative Test Refiner)**：拿到初始生成的测试后，ChatTester引入一个**“验证-修复”循环**。将测试编译运行，如果有错误，则将**错误信息和当前测试代码**反馈给ChatGPT，请它**修改代码直至通过**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。这和ChatUniTest后半部分类似，但ChatTester完全依赖ChatGPT自身来修复，不用手工规则。ChatGPT通过解析报错，很大程度上能修正自己的输出。此过程可多次迭代，直到测试编译通过且断言正确，或者达到最大迭代次数。

**改进效果**：作者在另一组新的Java方法上测试ChatTester，与默认ChatGPT生成比较，结果**显著提升**了测试正确率[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)：

- **编译通过率**提高了**34.3%**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)（相对24.5%的基线，提高到约32.9%，成功率增加三分之一以上）。这意味着ChatTester能让更多测试至少能运行起来。
- **正确断言的测试**增加了**18.7%**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)。也就是说，即使在编译通过的测试中，ChatTester生成的更正确，不会出现断言逻辑错误导致的失败，比默认ChatGPT多出近五分之一正确测试。
- 这些改进直接缩小了ChatGPT与完美测试之间的差距。如果说默认ChatGPT只有1/4有用，现在ChatTester约有**1/3以上**生成测试可直接使用，大大增强了LLM方法的实用性[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)。
- **保持人写风格**：重要的是，ChatTester虽然增加了步骤，但**并未牺牲生成测试的可读性**和覆盖率。生成的测试依然和ChatGPT原生输出一样保持了人类风格，且覆盖率等同，因为还是由ChatGPT撰写测试代码[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=,tests could be further improved)。开发者在用户研究中也认可，ChatTester改进的是底层正确性，测试样式依旧是他们喜欢的那种。

**论文贡献**：

- **全面评估**：首次从多角度（正确性、覆盖、可读性、偏好）评估了ChatGPT自动生成测试的能力，**量化**了其优缺点[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=In this work%2C we perform,be very promising if the)。这为社区了解LLM在测试生成方面的现状提供了宝贵数据点。
- **错误分析**：分类总结了ChatGPT生成测试的典型错误（例如未导入库、断言写错等），为有针对性改进提供依据。
- **ChatTester框架**：提出了一个**让LLM自我改进**输出质量的方法，将**模型输出-反馈-再输出**机制应用于测试代码生成[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。尤其是“意图提示”步骤，将代码生成任务**分解**为理解和生成两步，这是一种新颖的提示策略，验证了可以**通过巧妙Prompt设计提升ChatGPT对代码的理解和正确性**。
- **显著提升LLM测试可用性**：ChatTester在不重新训练模型的前提下，通过提示工程和多轮对话，把ChatGPT生成测试的可用率提高了约50%（从25%增至33%+）。这表明许多原先报错的测试并非LLM能力不足，而是需要稍加引导和校正，具有很强的启发意义。

**不足与展望**：

- **仍有失败案例**：尽管ChatTester减少了错误，但还有许多测试无法自动修复完全。例如一些逻辑错误ChatGPT可能反复修改仍不正确（可能需要更复杂推理或外部知识）。如何进一步提升成功率，甚至达到80-90%可用，这是未来挑战。
- **提示交互代价**：ChatTester需要多次API调用（理解+生成+可能数轮修复），相比一次性生成成本更高。在大批量生成时，时间和经济成本要考虑。不过可以平行化或仅对失败的部分做修复来减小开销。
- **覆盖率不是重点**：ChatTester关注正确性提升，对覆盖率并无专门优化。因此在覆盖要求极高的场景下（比如想逼近100%覆盖），ChatTester不一定提供额外帮助，需要结合如HITS等方法。另外，他们评估中ChatGPT本身覆盖率已不低，但如果有特定未覆盖分支，ChatTester框架没有机制去探索新的测试，主要聚焦“纠正已有测试”。
- **模型泛化**：ChatTester基于ChatGPT，未尝试其他LLM。原理上可以用于类似的对话LLM（如Claude等），但效用未验证。此外，随着模型升级（如GPT-4性能更好），ChatTester策略是否需要调整也值得研究。
- **断言质量**：虽然ChatTester增加了正确断言的数量，但LLM断言质量是否全面（比如检查边界情况、异常情况）没有深入讨论。ChatGPT倾向于生成“典型”断言，有时过于宽松或无效。未来或可加入*性质验证*等技术提高断言的有效性。

总体而言，该研究为初学者展示了如何**评估LLM工具**以及通过**提示工程改进LLM输出**的范例。ChatGPT生成测试有潜力但需辅以引导，ChatTester的成功证明了“让模型自己改进自己”是提升软件工程任务LLM应用的有效手段[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)。

# 横向对比分析 🔍

通过以上总结，我们可以对五篇论文的方法和成果进行**横向比较**，以加深对LLM用于单元测试生成的理解。

## 核心方法差异

- **传统 vs LLM vs 混合**：Abdullin等【2025】的研究直接对比了三类方法，无新算法提出但框架全面[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=techniques such as search,based approaches)。Chen等【2024】（ChatUniTest）和 Wang等【2024】（HITS）属于**纯LLM驱动框架**，在LLM提示和生成流程上引入新策略（上下文聚焦、方法切片）提升性能[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatGPT,Repair to address challenging errors)[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)。Lemieux等【2023】（CodaMOSA）则是**混合SBST+LLM**，让LLM作为辅助工具介入搜索[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=help SBST escape its coverage,operations and fitness function to)。Yuan等【2024】（ChatTester）利用**ChatGPT自身**进行改进，强调模型多轮交互提升输出质量[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。
- **粒度不同**：大部分LLM方法（ChatUniTest、TestSpark等）以**类或方法级**一次性生成完整测试类。HITS则**进一步细化到方法内部的片段**，粒度更细，每片生成局部测试然后组合[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)。CODAMOSA介入粒度是在**测试序列级**（当搜索停滞时生成新的测试序列注入）[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=help SBST escape its coverage,operations and fitness function to)。ChatTester解构了**提示步骤**，先让模型理解后再生成，相当于划分了任务的认知过程。
- **反馈机制**：ChatUniTest和ChatTester都采用**生成-验证-修复**循环，区别在于前者结合了**规则修复**和LLM修复[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=defined maximum prompt token limit,Repair to address challenging errors)[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=component includes a parser%2C compiler%2C,If a test remains)，而后者完全依赖ChatGPT多轮对话修复[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。TestSpark类似也有反馈环（如编译失败时再提示），CODAMOSA则属于**一次性生成后靠SBST变异改进**。HITS有简要提到fixer修复，但主要创新不在此而在切片。
- **场景特定优化**：HITS针对**复杂方法**场景提出特化方案，CODAMOSA针对**SBST停滞**场景提出调用LLM。ChatUniTest和ChatTester在**通用场景**下改进LLM生成质量，没有特别区分输入特性（但ChatUniTest观察到大类性能差，也在探索混合或切片方向）。Test Wars仅作对比分析，不提出新生成策略。

## 覆盖率与可执行性比较

- **覆盖率**：在传统方法中，SBST（EvoSuite）通常达到**较高覆盖率**（很多基准上>80%甚至90%）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=2011 ,on their datasets)。符号执行在小规模代码上可高覆盖，但复杂项目易因路径爆炸受限[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=efficient and effective ,did not consider these techniques)。LLM方法的平均覆盖率近期有迅速提升：ChatUniTest报告平均分支覆盖90.6%[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=2011 ,on their datasets)，HITS更是在复杂方法上弥补LLM短板，实现对传统方法的反超[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。Test Wars数据则显示ChatGPT-4变体（TestSpark)覆盖率仍略低于EvoSuite（38% vs 46%行覆盖）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=match at L692 In terms,However%2C we)。CODAMOSA聚焦Python模块，提升了约10%覆盖率，使很多模块覆盖进一步提高到新的水平[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)。ChatTester并未专注提升覆盖，但观察到ChatGPT生成的通过测试覆盖率**不输手工测试**[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=,tests could be further improved)，表明LLM有潜力达到高覆盖，只是失败测试浪费了潜能。综上，在**平均覆盖面**上，**最新LLM框架（ChatUniTest、HITS）已能接近或超过传统SBST**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=2011 ,on their datasets)[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)；混合方法（CODAMOSA）进一步突破瓶颈；而**基础LLM**（未经改进的ChatGPT）大致可达一定覆盖但不稳定。
- **可执行性（生成测试的可编译可运行率）**：传统工具生成的测试通常**100%可编译运行**，因为例如EvoSuite内置避免语法错误，符号执行本质上确保可执行路径。Test Wars中Kex编译成功率99.99%，EvoSuite也几乎全通过[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=These graphs give us multiple,out of all three)。LLM生成则容易出现错误代码：ChatGPT直接生成测试**仅约1/4完全可用**[mingwei-liu.github.io](https://mingwei-liu.github.io/assets/pdf/arxiv2023-chattester.pdf#:~:text=terms of correctness%2C sufficiency%2C readability%2C,tests could be further improved)。ChatUniTest通过修复循环将**成功率提升到30%\**以上[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=unfixed after several validation and,it is marked as deprecated)。ChatTester进一步提高了\**编译通过率到约33%**[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=unit test generation approach%2C which,assertions than the default ChatGPT)。HITS报告其**平均通过率**比其他LLM方案更高，减少了未编译的情况[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Additionally%2C we report the number,based)。换言之，**引入验证修复机制**（ChatUniTest、ChatTester）或**对LLM任务切分**（HITS）都有效提高了测试可执行性。CODAMOSA因LLM输出经过筛选插入SBST，其生成测试都经过SBST变异调整，一般也能执行，无额外错误。
- **错误类型**：LLM生成测试常见错误包括：未导包/符号未定义、类型不匹配导致编译失败，或断言逻辑错误导致运行期失败[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)。这些通过规则或反馈修复可解决相当一部分[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=)。TestSpark/ChatUniTest等也遇到类似问题并进行了处理。SBST生成测试虽可运行，但常有**无效断言**（如缺乏验证，仅空assert）的问题，属于“通过率100%但检错率低”的另一种可用性问题，这在LLM生成测试中反而较少出现，因为LLM倾向于生成有意义断言[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=the unit tests generated by,learning model%2C leading to most)。

## 针对复杂方法 & 生成质量提升技巧

- **复杂方法覆盖**：这是LLM方法的普遍弱项。ChatUniTest未对复杂方法特意处理，导致复杂度高的方法覆盖率骤降50%以上[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Table 1,61.11)。HITS正是为此设计，对复杂方法进行切片显著提高了覆盖，几乎弥补了LLM在此的不足[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=slice,line and branch coverage scores)。CODAMOSA从另一角度，通过SBST先探索简单路径，LLM再处理困难输入，也间接有助复杂场景的覆盖。总体看，**复杂逻辑需要额外信息或分解**才能充分覆盖：HITS用静态分析+分步提示，CODAMOSA借助SBST指导，都是让LLM**不要盲目硬碰整个复杂问题**，而是有所依托。
- **提升生成质量技巧**：
  - *上下文优化*：ChatUniTest的**自适应焦点上下文**确保LLM获取充分而不过载的信息，防止因上下文截断而遗漏关键点[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatUniTest introduces an Adaptive Focal,avoiding truncated responses from ChatGPT)。这对大类/方法效果明显，比简单截断或无选择地提供代码要强。
  - *链式思维提示*：HITS运用了**Chain-of-Thought**，引导LLM分步骤思考测试生成[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=slice of the focal method,to instruct the large language)。ChatTester的“先意图后代码”提示也属于一种CoT风格，将任务分解[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。这些技巧本质上帮助LLM**理清思路**、减少一步到位的失误。
  - *反馈回路*：多轮生成-修复已被证明极大提高了正确率和质量。无论是ChatUniTest、ChatTester还是TestSpark，都强调了**验证输出并反馈**给模型再尝试的重要性[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=component includes a parser%2C compiler%2C,If a test remains)[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,assertions than the default ChatGPT)。LLM善于对提示做出改进建议，自然适合这种自我纠错模式。
  - *融合其他技术*：CODAMOSA通过**融合搜索算法**，利用SBST的系统探索能力补足LLM随机性，对付特殊输入效果佳[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=Recently%2C large language models ,Codex to generate tests for)。未来可能还有**符号执行+LLM**等混合，以发挥各自所长。
  - *模型选择和提示调优*：Test Wars显示不同LLM质量有别：ChatGPT-4（有代码调优的版本）比Code Llama等开源模型生成测试效果好很多（覆盖率和编译率均领先）[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Figure 3 shows the average compilation,Other models show)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=ChatGPT,4o)。因此**选择更强的模型**本身也是提升质量的捷径。另外，提示词模板、Few-shot示例等调优在论文中未深入，但在实践中也很关键。

## 使用的模型和工具

- **LLM模型**：大多数工作使用了**OpenAI系模型**。ChatUniTest和ChatTester明确用ChatGPT（GPT-3.5/GPT-4，不同版本)[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=implements core workflow%2C complemented by,available at this https URL)[arxiv.org](https://arxiv.org/abs/2305.04207#:~:text=,still suffer from correctness issues)。TestSpark在Test Wars中也试了ChatGPT-4、开源Llama等[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=TestSpark,a dataset of recent Java)。CODAMOSA用的是OpenAI Codex（code-davinci）[conf.researchr.org](https://conf.researchr.org/details/icse-2023/icse-2023-technical-track/132/CodaMOSA-Escaping-Coverage-Plateaus-in-Test-Generation-with-Pre-trained-Large-Langua#:~:text=,covered functions)。HITS未明说模型名称，但很可能用ChatGPT（因为引用了ChatUniTest、ChatTester结果）。可以认为目前**GPT-4**系列是测试生成效果最好的模型[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=Figure 3 shows the average compilation,Other models show)，许多框架选其为后端。也有学者尝试过CodeT5、BART等fine-tune模型（如AthenaTest用BART微调[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=the unit tests generated by,learning model%2C leading to most)），但这些在新工作中逐渐被更强大的通用LLM取代。
- **工具集成**：ChatUniTest和TestSpark都集成到了**IntelliJ IDEA插件**中，方便开发者使用[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=TestSpark ,improve the initial LLM response)。EvoSuite、Pynguin、Kex是研究界成熟工具。CODAMOSA实现于Pynguin的插件。ChatTester目前是原型脚本形式（有GitHub实现[github.com](https://github.com/jstzwj/ChatTester#:~:text=jstzwj%2FChatTester%3A No More Manual Tests%3F,(arxiv)）。总体看，**LLM测试生成工具正在走向工程集成**：IDE插件、命令行工具等开始出现，让这项技术更易用。
- **Symbolic Prompt**: 除上述，值得一提的是有些方法使用**符号信息**改进提示，例如SymPrompt方法使用代码符号信息构造提示针对方法级测试[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=improve the initial LLM response)。本次列举的论文中提及SymPrompt和AID等特别案例[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=improve the initial LLM response)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=AID [32] is a LLM,purpose unit test generation)，但它们侧重于方法级或特殊场景，不在五篇主文之列。

## 扩展性与工程化程度

- **扩展性**：指方法能否推广到更多语言、项目、模型。**混合策略**（CODAMOSA）依赖具体SBST实现，目前限于Python/Pynguin，但思路可用在Java+EvoSuite等，只需解决不同语言LLM提示和测试插桩问题。**LLM框架**（ChatUniTest、HITS）理论上语言无关，但需各语言的解析和测试执行支持；ChatUniTest用Java AST和JUnit，若移植到Python需替换为PyAST和pytest等。ChatTester的方法更普适，只要有ChatGPT API和编译器就能用在任何语言（论文用了Java，也可设想C#、Python类似应用）。
- **工程化**：ChatUniTest在FSE以**演示工具**发表，附有GitHub和演示视频，说明已经到**实用工具**阶段[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=effectiveness evaluation reveals that ChatUniTest,available at this https URL)。TestSpark亦在ICSE以**工具演示**形式发布，有IDE插件成品[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=TestSpark ,improve the initial LLM response)。CODAMOSA代码开源（微软研究开源推动），适合研究人员复现。HITS作为ASE论文估计也会开源工具以便社区测试。ChatTester的实现已在GitHub开放，可供开发者试用改进[github.com](https://github.com/jstzwj/ChatTester#:~:text=jstzwj%2FChatTester%3A No More Manual Tests%3F,(arxiv)。可以说，这几项工作**工程成熟度都较高**，大多提供了可用的原型或插件，便利后续研究和实践采纳。
- **可融合性**：多项研究提到**未来考虑混合**。Test Wars作者计划将LLM、SBST、符号执行三者优势结合[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=II)。ChatUniTest、HITS等LLM框架也可看作模块化组件，完全可能与SBST/符号技术组合（如先用LLM生成部分测试，再用SBST补充优化）。因此这些方法并不互斥，**组合拳**或许是下一步趋势。
- **现实应用前景**：ChatUniTest用户研究已表明实际开发者对LLM生成测试表现出兴趣[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=effectiveness evaluation reveals that ChatUniTest,https URL%2C and the demo)。一旦正确性问题受控（ChatTester方向的努力），LLM测试有望**减少手工测试编写**。同时，用LLM生成测试也可反过来**辅助调试代码**（测试失败则代码有问题）。JetBrains等公司投入该领域（TestSpark即JetBrains研究产物[apanichella.github.io](https://apanichella.github.io/publication/#:~:text=A,Demonstration 2024)），预示这些技术有望集成到主流IDE中，为开发者提供“一键生成单元测试”的功能。

综上，这五篇论文共同描绘了**LLM+测试生成**领域的最新进展。从中我们看到了LLM的强大（生成更语义化的测试、潜在提高变异检错率[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)），也认识到其局限（覆盖盲区、偶尔幻觉错误）。通过不同创新手段——上下文聚焦、分而治之、反馈循环、混合搜索——研究者正逐步克服这些挑战，使得“让AI替我们写单元测试”这一愿景愈发接近现实✨。

**参考文献**（按照上述论文顺序）：

1. Azat Abdullin, et al. *Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation*. ICST 2025.[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=tools’ performance on the GitBug,based approaches being)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2501.10200#:~:text=We identify three critical limitations,partially tackle)
2. Yinghao Chen, et al. *ChatUniTest: A Framework for LLM-Based Test Generation*. arXiv 2024 / FSE 2024 (Demo).[ar5iv.org](https://ar5iv.org/abs/2305.04764v1#:~:text=ChatGPT,Repair to address challenging errors)[arxiv.org](https://arxiv.org/abs/2305.04764#:~:text=implements core workflow%2C complemented by,available at this https URL)
3. Caroline Lemieux, et al. *CodaMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models*. ICSE 2024.[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=help SBST escape its coverage,operations and fitness function to)[carolemieux.com](https://www.carolemieux.com/codamosa_icse23.pdf#:~:text=We find that CODAMOSA achieves,functions%2C and introduce syntactical 1)
4. Zejun Wang, et al. *HITS: High-coverage LLM-based Unit Test Generation via Method Slicing*. ASE 2024.[arxiv.org](https://arxiv.org/abs/2408.11324#:~:text=missing lines and branches,line and branch coverage scores)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2408.11324v1#:~:text=Table 1,61.11)
5. Zhiqiang Yuan, et al. *No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation*. arXiv 2023/2024.[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=usability,tests could be further improved)[ar5iv.org](https://ar5iv.org/html/2305.04207v3#:~:text=Inspired by our findings above%2C,assertions than the default ChatGPT)